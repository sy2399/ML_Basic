{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:20:55.156571Z",
     "start_time": "2020-02-11T09:20:51.908725Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.8\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 880, got 864\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# keras.__version__\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:20:56.444480Z",
     "start_time": "2020-02-11T09:20:55.206756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x72 with 36 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.figure(figsize=(6, 1))\n",
    "for i in range(36):\n",
    "    plt.subplot(3, 12, i+1)\n",
    "    plt.imshow(X_train0[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:20:56.694116Z",
     "start_time": "2020-02-11T09:20:56.498486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) float32\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train0.reshape(60000, 784).astype('float32') / 255.0\n",
    "X_test = X_test0.reshape(10000, 784).astype('float32') / 255.0\n",
    "print(X_train.shape, X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:20:56.761887Z",
     "start_time": "2020-02-11T09:20:56.751398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train0, 10)\n",
    "Y_test = to_categorical(y_test0, 10)\n",
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Data & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:20:56.903713Z",
     "start_time": "2020-02-11T09:20:56.856211Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(X_train.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:20:56.996739Z",
     "start_time": "2020-02-11T09:20:56.989312Z"
    }
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 의 문제점\n",
    "\n",
    "- Vanishing Gradient\n",
    "- Overfitting \n",
    "\n",
    "\n",
    "-  해결방법: Geoffrey Hinton’s summary of finding up to today\n",
    "    1. Our labeled datasets were thousands of times too small\n",
    "    2. Out computers were millions of times too slow\n",
    "    3. We initialized the weight in a stupid way\n",
    "    4. We used the wrong type of non-linearity\n",
    "    \n",
    "    \n",
    "-> Another Activation Function (e.g., ReLU)\n",
    "    \n",
    "->  Weight Initialization (e.g., RBM, Xavier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Simple - Softmax (92.07%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:04:15.929550Z",
     "start_time": "2020-02-11T09:03:27.145559Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 8.349071570\n",
      "Epoch: 0002, Cost: 3.945423525\n",
      "Epoch: 0003, Cost: 2.476440671\n",
      "Epoch: 0004, Cost: 1.993275747\n",
      "Epoch: 0005, Cost: 1.567288953\n",
      "Epoch: 0006, Cost: 1.315574946\n",
      "Epoch: 0007, Cost: 1.187640692\n",
      "Epoch: 0008, Cost: 1.153053203\n",
      "Epoch: 0009, Cost: 0.975832514\n",
      "Epoch: 0010, Cost: 0.880599426\n",
      "Epoch: 0011, Cost: 0.881525149\n",
      "Epoch: 0012, Cost: 0.756160670\n",
      "Epoch: 0013, Cost: 0.856528202\n",
      "Epoch: 0014, Cost: 0.812715300\n",
      "Epoch: 0015, Cost: 0.745195524\n",
      "Epoch: 0016, Cost: 0.706151605\n",
      "Epoch: 0017, Cost: 0.675870856\n",
      "Epoch: 0018, Cost: 0.614093752\n",
      "Epoch: 0019, Cost: 0.680277948\n",
      "Epoch: 0020, Cost: 0.595901807\n",
      "Epoch: 0021, Cost: 0.584149028\n",
      "Epoch: 0022, Cost: 0.640533935\n",
      "Epoch: 0023, Cost: 0.615513751\n",
      "Epoch: 0024, Cost: 0.511086659\n",
      "Epoch: 0025, Cost: 0.550201741\n",
      "Epoch: 0026, Cost: 0.558958994\n",
      "Epoch: 0027, Cost: 0.545070681\n",
      "Epoch: 0028, Cost: 0.480769500\n",
      "Epoch: 0029, Cost: 0.479089481\n",
      "Epoch: 0030, Cost: 0.567238196\n",
      "Epoch: 0031, Cost: 0.574723486\n",
      "Epoch: 0032, Cost: 0.479385910\n",
      "Epoch: 0033, Cost: 0.490376272\n",
      "Epoch: 0034, Cost: 0.537102287\n",
      "Epoch: 0035, Cost: 0.451561030\n",
      "Epoch: 0036, Cost: 0.501343405\n",
      "Epoch: 0037, Cost: 0.537714361\n",
      "Epoch: 0038, Cost: 0.503326276\n",
      "Epoch: 0039, Cost: 0.480123194\n",
      "Epoch: 0040, Cost: 0.449382593\n",
      "Epoch: 0041, Cost: 0.454734581\n",
      "Epoch: 0042, Cost: 0.461344849\n",
      "Epoch: 0043, Cost: 0.460314137\n",
      "Epoch: 0044, Cost: 0.434237419\n",
      "Epoch: 0045, Cost: 0.469638722\n",
      "Epoch: 0046, Cost: 0.476424040\n",
      "Epoch: 0047, Cost: 0.458929487\n",
      "Epoch: 0048, Cost: 0.465955329\n",
      "Epoch: 0049, Cost: 0.361943980\n",
      "Epoch: 0050, Cost: 0.487045108\n",
      "Learning Finished!\n",
      "Accuracy: 0.9027\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "\n",
    "#AdapOptimizer \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = next_batch(5, X_train, Y_train)\n",
    "\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: X_test, Y: Y_test}),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, X_test.shape[0] - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(Y_test[r : r + 1], axis=1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(\n",
    "            tf.argmax(hypothesis, axis=1), feed_dict={X: X_test[r : r + 1]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        X_test[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN for MNIST (91.6%)\n",
    "\n",
    "위의 simple 과 다른점!\n",
    "- 3개의 layer \n",
    "- ReLU Activation function\n",
    "- AdamOptimzer 사용 \n",
    "    - GradientDescnt 사용시 Cost 가 너무 높아!\n",
    "    - <span class=\"mark\">그 이유는????</span>\n",
    "- 그래도 simple 보다 성능이 낮다!!\n",
    "    - Vanishing gradient, Overfitting 의 문제점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:24:41.023315Z",
     "start_time": "2020-02-11T09:24:19.414569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 305.548692740\n",
      "Epoch: 0002 cost = 103.726367054\n",
      "Epoch: 0003 cost = 73.024195420\n",
      "Epoch: 0004 cost = 58.290952824\n",
      "Epoch: 0005 cost = 41.144590531\n",
      "Epoch: 0006 cost = 41.947407991\n",
      "Epoch: 0007 cost = 38.971400772\n",
      "Epoch: 0008 cost = 36.178555730\n",
      "Epoch: 0009 cost = 31.196481701\n",
      "Epoch: 0010 cost = 27.525687645\n",
      "Epoch: 0011 cost = 31.201659956\n",
      "Epoch: 0012 cost = 25.780164402\n",
      "Epoch: 0013 cost = 20.096388386\n",
      "Epoch: 0014 cost = 20.611374239\n",
      "Epoch: 0015 cost = 18.889040844\n",
      "Learning Finished!\n",
      "Accuracy: 0.916\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=Y\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(X_train.shape[0] / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = next_batch(5, X_train, Y_train)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: X_test, Y: Y_test}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, X_test.shape[0] - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(Y_test[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: X_test[r:r + 1]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with Xavier for MNIST (95.7%)\n",
    "\n",
    "- Weight 초기화\n",
    "    - 기존 : tf.random_normal()\n",
    "    - Xavier : initializer=tf.contrib.layers.xavier_initializer())\n",
    "         - epoch 1 부터 cost 가 낮음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:25:33.549594Z",
     "start_time": "2020-02-11T09:25:33.546713Z"
    }
   },
   "outputs": [],
   "source": [
    "##Variable w1 already exists, disallowed.\n",
    "\n",
    "#1. 이전에 get_variable 로 생성된 변수 삭제\n",
    "tf.reset_default_graph()\n",
    "\n",
    " \n",
    "#2. 특정 공간 내에서만 변수가 유효하도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:25:33.689152Z",
     "start_time": "2020-02-11T09:25:33.686296Z"
    }
   },
   "outputs": [],
   "source": [
    "xavier_initializer = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:25:55.997008Z",
     "start_time": "2020-02-11T09:25:33.851010Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.666920797\n",
      "Epoch: 0002 cost = 0.333713752\n",
      "Epoch: 0003 cost = 0.270432347\n",
      "Epoch: 0004 cost = 0.199046450\n",
      "Epoch: 0005 cost = 0.200219855\n",
      "Epoch: 0006 cost = 0.205369528\n",
      "Epoch: 0007 cost = 0.170421803\n",
      "Epoch: 0008 cost = 0.185998726\n",
      "Epoch: 0009 cost = 0.151544996\n",
      "Epoch: 0010 cost = 0.161922995\n",
      "Epoch: 0011 cost = 0.137975735\n",
      "Epoch: 0012 cost = 0.146844062\n",
      "Epoch: 0013 cost = 0.133591782\n",
      "Epoch: 0014 cost = 0.111300988\n",
      "Epoch: 0015 cost = 0.129386781\n",
      "Learning Finished!\n",
      "Accuracy: 0.9571\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=xavier_initializer)\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=xavier_initializer)\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=xavier_initializer)\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(X_train.shape[0]/ batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = next_batch(5, X_train, Y_train)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: X_test, Y: Y_test}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, X_test.shape[0] - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(Y_test[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: X_test[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN - Deep for MNIST (97.42%)\n",
    "\n",
    "- Xavier\n",
    "- 5 Layer\n",
    "- 512 node\n",
    "\n",
    "    - Xavier 만 사용했을 때보다 낮은 정확도!\n",
    "    - Problem : Overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:26:36.413927Z",
     "start_time": "2020-02-11T09:26:36.411282Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T09:27:35.791830Z",
     "start_time": "2020-02-11T09:26:51.682388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.812612216\n",
      "Epoch: 0002 cost = 0.425028810\n",
      "Epoch: 0003 cost = 0.349618246\n",
      "Epoch: 0004 cost = 0.293401256\n",
      "Epoch: 0005 cost = 0.280735502\n",
      "Epoch: 0006 cost = 0.283322707\n",
      "Epoch: 0007 cost = 0.271189488\n",
      "Epoch: 0008 cost = 0.237625792\n",
      "Epoch: 0009 cost = 0.221558850\n",
      "Epoch: 0010 cost = 0.259535939\n",
      "Epoch: 0011 cost = 0.180214634\n",
      "Epoch: 0012 cost = 0.252304744\n",
      "Epoch: 0013 cost = 0.207063755\n",
      "Epoch: 0014 cost = 0.184041029\n",
      "Epoch: 0015 cost = 0.190609413\n",
      "Learning Finished!\n",
      "Accuracy: 0.9498\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(X_train.shape[0]/ batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = next_batch(5, X_train, Y_train)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: X_test, Y: Y_test}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, X_test.shape[0] - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(Y_test[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: X_test[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with Dropout for MNIST (98.04%)\n",
    "\n",
    "- Overfitting 방지!\n",
    "\n",
    "- tf.nn.dropout(Layer1, keep_prob = keep_prob)\n",
    "    - keep_prob : 전체 중 몇 % 를 keep할건지\n",
    "    \n",
    "    - train : 0.5 ~ 0.7\n",
    "    - test : 1 (알고 있는 모든 것을 총 동원!)\n",
    "    \n",
    "+ Optimizer\n",
    "    - 기존 : Gradient Descent\n",
    "    - new : Adam, SGD, Adagrad, RMSProp....\n",
    "    - optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Bach Normalizatoin for MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "329px",
    "left": "443px",
    "right": "20px",
    "top": "120px",
    "width": "325px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
