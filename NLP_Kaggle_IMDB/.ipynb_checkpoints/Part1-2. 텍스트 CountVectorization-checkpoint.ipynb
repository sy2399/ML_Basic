{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model\n",
    "\n",
    "[위키피디아](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "다음의 두 문장이 있다고 하자,\n",
    "\n",
    "(1) John likes to watch movies. Mary likes movies too.   \n",
    "(2) John also likes to watch football games.   \n",
    "\n",
    "위 두 문장을 토큰화 하여 가방에 담아주면 다음과 같다.\n",
    "\n",
    "[\n",
    "    \"John\",\n",
    "    \"likes\",\n",
    "    \"to\",\n",
    "    \"watch\",\n",
    "    \"movies\",\n",
    "    \"Mary\",\n",
    "    \"too\",\n",
    "    \"also\",\n",
    "    \"football\",\n",
    "    \"games\"\n",
    "]\n",
    "\n",
    "\n",
    "그리고 배열의 순서대로 가방에서 각 토큰이 몇 번 등장하는지 횟수를 세어준다.\n",
    "\n",
    "(1) [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]   \n",
    "(2) [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]   \n",
    "\n",
    "=> **머신러닝 알고리즘이 이해할 수 있는 형태**로 바꿔주는 작업이다.\n",
    "\n",
    "***\n",
    "단어 가방을 n-gram을 사용해 bigram 으로 담아주면 다음과 같다.\n",
    "\n",
    "[\n",
    "    \"John likes\",\n",
    "    \"likes to\",\n",
    "    \"to watch\",\n",
    "    \"watch movies\",\n",
    "    \"Mary likes\",\n",
    "    \"likes movies\",\n",
    "    \"movies too\",\n",
    "]\n",
    "\n",
    "=> 여기에서는 CountVectorizer를 통해 위 작업을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn 의 CountVectorizer 를 통해 Featuer 생성\n",
    "\n",
    "- 정규표현식을 사용해 토큰을 추출한다.\n",
    "- 모두 소문자로 변환시키기 때문에 good, Good, gOod이 모두 같은 특성이 된다.\n",
    "- 의미없는 특성을 많이 생성하기 때문에 적어도 두 개의 문서에 나타난 토큰만을 사용한다. \n",
    "    - *min_df* 파라미터로 토큰이 나타날 최소 문서 개수를 지정할 수 있다.\n",
    "\n",
    "***\n",
    "**CountVectorizer**\n",
    ": 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩한 벡터를 만든다.\n",
    "- 문서를 토큰 리스트로 변환한다.\n",
    "- 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "- 각 문서를 BOW 인코딩 벡터로 변환한다.\n",
    "\n",
    "*parameter*\n",
    "- stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "    - stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "    - 리스트 형태로 불용어로 처리하고자 하는 문자를 넣어준다\n",
    "<br><br>   \n",
    "- analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "    - 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "    - 문자열 또는 함수로 어떤 단위로 토큰화 할지 정의\n",
    "<br><br>        \n",
    "- token_pattern : string\n",
    "    - 토큰 정의용 정규 표현식\n",
    "<br><br>       \n",
    "- tokenizer : 함수 또는 None (디폴트)\n",
    "    - 토큰 생성 함수 .\n",
    "<br><br>        \n",
    "- ngram_range : (min_n, max_n) 튜플\n",
    "    - n-그램 범위\n",
    "<br><br>        \n",
    "- max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "    - 단어장에 포함되기 위한 최대 빈도\n",
    "<br><br>        \n",
    "- min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "    - 단어장에 포함되기 위한 최소 빈도\n",
    "<br><br>        \n",
    "- max_features : build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "[Scikit-Learn 의 문서 전처리 기능](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:21:57.635469Z",
     "start_time": "2020-05-14T08:21:57.556328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        stuff go moment mj start listen music watch od...\n",
       "1        classic war world timothi hine entertain film ...\n",
       "2        film start manag nichola bell give welcom inve...\n",
       "3        must assum prais film greatest film opera ever...\n",
       "4        superbl trashi wondrous unpretenti exploit hoo...\n",
       "                               ...                        \n",
       "24995    seem like consider gone imdb review film went ...\n",
       "24996    believ made film complet unnecessari first fil...\n",
       "24997    guy loser get girl need build pick stronger su...\n",
       "24998    minut documentari bu uel made earli one spain ...\n",
       "24999    saw movi child broke heart stori unfinish end ...\n",
       "Name: review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part1-1 에서 전처리한 텍스트 데이터\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open('p1_clean_train_reviews.p', 'rb') as file:\n",
    "    clean_train_reviews = pickle.load(file)\n",
    "    \n",
    "with open('p1_clean_test_reviews.p', 'rb') as file:\n",
    "    clean_test_reviews = pickle.load(file)\n",
    "\n",
    "clean_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:01:02.063017Z",
     "start_time": "2020-05-14T08:01:02.047511Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:08:57.679833Z",
     "start_time": "2020-05-14T08:08:57.673925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=20000, min_df=2, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 튜토리얼과 다르게 파라메터 값을 수정\n",
    "# 파라메터 값만 수정해도 캐글 스코어 차이가 많이 남\n",
    "vectorizer = CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             min_df = 2, # 토큰이 나타날 최소 문서 개수\n",
    "                             ngram_range=(1, 3), \n",
    "                             max_features = 20000 \n",
    "                            )\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. 속도 개선을 위해 pipeline 사용   \n",
    "참고 : https://stackoverflow.com/questions/28160335/plot-a-document-tfidf-2d-graph\n",
    "\n",
    "\n",
    "- pipeline 에서 메모리의 캐시를 사용할 수 있다는 문구도 있습니다. 하지만 성능상의 이슈라기보다는 cross-validation 과 GridSearch 과정을 하나로 만들어 주는게 pipeline의 가장 큰 장점이라고 합니다. \n",
    "\n",
    "- Countvectorizer 사용시 하나의 과정만 묶어줘서 사실 굳이 pipeline을 쓸 필요는 없는데 보통 transform 하는 과정에서 pipeline을 사용합니다.\n",
    "\n",
    "- 아래의 코드에선 cross-validation 과 GridSearch 과정이 없는데 해당 과정은 모델의 성능(정확도)를 측정해 보는 과정이고, GridSearch는 최적의 하이퍼파라메터를 찾는 과정입니다.\n",
    "\n",
    "- Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "- 더 읽어보기 : https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:01:19.514806Z",
     "start_time": "2020-05-14T08:01:19.511982Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 fit을 학습하는데 사용하는데 여기에서는 벡터화 할 때도 fit을 사용했습니다. 사이킷런에 구현된 벡터화 알고리즘이 fit을 사용해서 벡터화 하도록 되어 있습니다. 따라서 RF에서의 fit과 벡터화에 사용되는 fit은 다른 성격입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:22:13.978404Z",
     "start_time": "2020-05-14T08:22:01.517397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 585 ms, total: 12.4 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%time train_data_features = pipeline.fit_transform(clean_train_reviews)\n",
    "train_data_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
